behaviors:
  HierarchicalAgent-LSTM:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0002
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 3
      vis_encode_type: simple
      goal_conditioning_type: hyper
      memory:
        memory_size: 256
        sequence_length: 64
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 3
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 20
    checkpoint_interval: 500000
    max_steps: 8000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false

  E2EAgent-LSTM:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0002
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 3
      vis_encode_type: simple
      goal_conditioning_type: hyper
      memory:
        memory_size: 256
        sequence_length: 64
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 3
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 20
    checkpoint_interval: 500000
    max_steps: 8000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false
  
  HierarchicalAgent-NonLSTM:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0002
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 3
      vis_encode_type: simple
      goal_conditioning_type: hyper
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 3
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 20
    checkpoint_interval: 500000
    max_steps: 8000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false

  E2EAgent-NonLSTM:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0002
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 3
      vis_encode_type: simple
      goal_conditioning_type: hyper
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 3
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 20
    checkpoint_interval: 500000
    max_steps: 8000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false

  FixedHierarchicalAgent-NonLSTM:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0002
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 3
      vis_encode_type: simple
      goal_conditioning_type: hyper
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 3
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 20
    checkpoint_interval: 500000
    max_steps: 8000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false


  HierarchicalAgent-Team:
    trainer_type: poca
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 
        opt_values: [0.0003, 0.0004]
      beta: 
        opt_values: [0.003, 0.004]
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: 
        opt_values: [linear, constant]
      epsilon_schedule: linear
      beta_schedule:
        opt_values: [linear, constant]
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
      goal_conditioning_type: hyper
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 256
          num_layers: 3
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 25
    checkpoint_interval: 500000
    max_steps: 16000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false
  
  HierarchicalAgent-TeamDOE:
    trainer_type: poca
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0003
      beta: 0.007
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      beta_schedule: linear
      epsilon_schedule: linear
      learning_rate_schedule: constant
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
      goal_conditioning_type: hyper
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 256
          num_layers: 3
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 25
    checkpoint_interval: 500000
    max_steps: 8000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false

  
  
  E2EAgent-Team:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0003
      beta: 0.01
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: constant
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 4
      vis_encode_type: simple
      goal_conditioning_type: hyper
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 4
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 20
    checkpoint_interval: 500000
    max_steps: 12000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false

  FixedHierarchicalAgent-Team:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512
      buffer_size: 10240
      learning_rate: 0.0003
      beta: 0.01
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: constant
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 4
      vis_encode_type: simple
      goal_conditioning_type: hyper
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 4
          vis_encode_type: simple
          goal_conditioning_type: hyper
    self_play:
      window: 10
      play_against_latest_model_ratio: 0.5
      save_steps: 20000
      swap_steps: 10000
      team_change: 100000
      initial_elo: 400
    keep_checkpoints: 20
    checkpoint_interval: 500000
    max_steps: 12000000
    time_horizon: 64
    summary_freq: 1000
    threaded: false
